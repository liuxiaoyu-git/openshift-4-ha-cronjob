apiVersion: v1
kind: Template
metadata:
  name: ha-cronjob
  labels:
    app: ha-cronjob
    template: ha-cronjob
parameters:
- displayName: This cluster API URL
  description: This clusters API URL (e.g. 'https://api.ocp4north.example.domain:6443')
  name: THIS_CLUSTER_API_URL
  required: true
- displayName: Alternate active cluster API URL List
  description: Comma separated list of alternate active cluster API URLs (e.g. 'https://api.ocp4east.example.domain:6443,https://api.ocp4west.example.domain:6443')
  name: ALTERNATE_CLUSTER_API_URL_LIST
  required: true
- displayName: Deployment or DeploymentConfig
  description: Whether the application is deployed via Deployment of DeploymentConfig object (e.g. deployment, deploymentconfig). Must be understood by the oc cli
  name: DEPLOYMENT_OR_DEPLOYMENTCONFIG
  value: deploymentconfig
- displayName: Application Deployment Name
  description: The name of the deployment / deploymentconfig object to scale
  name: DEPLOYMENT_NAME
  required: true
- displayName: Total desired replicas
  description: The total desired number of replicas running in all clusters. Works best when it is a multiple of all active clusters
  name: TOTAL_DESIRED_REPLICAS
  value: "6"
- displayName: Passive Cluster
  description: Whether this cluster is a passive cluster in an active-passive configuration
  name: PASSIVE_CLUSTER
  value: "false"
- displayName: Deployment project
  description: Project the application deployment resides in
  name: DEPLOYMENT_PROJECT
  required: true
- displayName: OpenShift Version
  description: Version of OpenShift. Must match http://mirror.openshift.com/pub/openshift-v3/clients tags (e.g. 3.11.157, 3.11.130)
  name: OC_VERSION
  value: "3.11.157"
objects:
# ImageStream
- apiVersion: image.openshift.io/v1
  kind: ImageStream
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}
    namespace: ${DEPLOYMENT_PROJECT}
    labels:
      app.kubernetes.io/name: ha-cronjob
      app.kubernetes.io/instance: ha-cronjob-${DEPLOYMENT_NAME}
      app.kubernetes.io/version: 1.0.0
      app.kubernetes.io/component: imagestream
      app.kubernetes.io/part-of: ${DEPLOYMENT_NAME}
      app.kubernetes.io/managed-by: openshift

# BuildConfig
- apiVersion: build.openshift.io/v1
  kind: BuildConfig
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}
    namespace: ${DEPLOYMENT_PROJECT}
    labels:
      app.kubernetes.io/name: ha-cronjob
      app.kubernetes.io/instance: ha-cronjob-${DEPLOYMENT_NAME}
      app.kubernetes.io/version: 1.0.0
      app.kubernetes.io/component: buildconfig
      app.kubernetes.io/part-of: ${DEPLOYMENT_NAME}
      app.kubernetes.io/managed-by: openshift
  spec:
    triggers:
    - type: ConfigChange
    source:
      type: Dockerfile
      dockerfile: |
        FROM registry.access.redhat.com/ubi8/ubi:latest

        # Install ansible and OpenShift cli
        RUN yum -y install python3-pip \
         && pip3 install ansible \
         && curl -o /tmp/oc.tar.gz https://mirror.openshift.com/pub/openshift-v3/clients/${OC_VERSION}/linux/oc.tar.gz \
         && tar xvf /tmp/oc.tar.gz -C /usr/bin \
         && rm -rf /tmp/oc.tar.gz

        # It is assumed the entrypoint will be configured in the job template
        ENTRYPOINT ["echo", "This image is not meant to be run directly :)"]
    strategy:
      type: Docker
    output:
      to:
        kind: ImageStreamTag
        name: ha-cronjob-${DEPLOYMENT_NAME}:latest

# ConfigMap
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}
    namespace: ${DEPLOYMENT_PROJECT}
    labels:
      app.kubernetes.io/name: ha-cronjob
      app.kubernetes.io/instance: ha-cronjob-${DEPLOYMENT_NAME}
      app.kubernetes.io/version: 1.0.0
      app.kubernetes.io/component: configmap
      app.kubernetes.io/part-of: ${DEPLOYMENT_NAME}
      app.kubernetes.io/managed-by: openshift
  data:
    job: |-
      #!/usr/bin/env ansible-playbook
      - hosts: localhost
        gather_facts: false
        vars:
          # This clusters API URL (e.g. "https://api.ocp4north.example.domain:6443")
          this_cluster_api_url: "{{ lookup('env', 'THIS_CLUSTER_API_URL') }}"

          # Array of cluster api urls of all active clusters
          alternate_clusters_array: "{{ lookup('env', 'ALTERNATE_CLUSTER_API_URL_LIST').split(',') }}"

          # Whether the application is deployed via Deployment of DeploymentConfig object (e.g. deployment, deploymentconfig). Must be understood by the oc cli
          deployment_or_deploymentconfig: "{{ lookup('env', 'DEPLOYMENT_OR_DEPLOYMENTCONFIG') }}"

          # The name of the deployment / deploymentconfig object to scale
          deployment_name: "{{ lookup('env', 'DEPLOYMENT_NAME') }}"

          # The namespace the application resides in
          project_name: "{{ lookup('env', 'PROJECT_NAME') }}"

          # How many application replicas running in all active clusters
          total_desired_replicas: "{{ lookup('env', 'TOTAL_DESIRED_REPLICAS') }}"

          # Whether this cluster is a passive cluster in an active-passive configuration
          passive_cluster: "{{ lookup('env', 'PASSIVE_CLUSTER') }}"

          # The service account the CronJob pod is running as
          sa_token: "{{ lookup('file', '/var/run/secrets/kubernetes.io/serviceaccount/token') }}"

        tasks:
        - name: Check alternate cluster statuses
          uri:
            url: "{{ item }}/healthz"
            validate_certs: no
            return_content: yes
          loop: "{{ alternate_clusters_array }}"
          failed_when: false
          register: get_cluster_statuses

        - name: Print cluster responses
          debug:
            msg:
            - "Cluster: {{ item.key }}"
            - "/healthz Response: {{ item.value }}"
          with_dict: "{{ dict(alternate_clusters_array| zip(get_cluster_statuses.results | map(attribute='content') | list)) }}"

        - name: Calculate cluster counts
          # We add 1 if active-active because we assume this cluster is healthy if this job is running
          # At worst this means we are accidentally running more replicas than we mean to
          # We don't add anything when active-passive
          set_fact:
            total_clusters: "{{ alternate_clusters_array
                              | count
                              + (passive_cluster | ternary(0, 1) | int) }}"
            healthy_clusters: "{{ get_cluster_statuses.results
                                | map(attribute='content')
                                | select('match', '^ok$')
                                | list
                                | count
                                + (passive_cluster | ternary(0, 1) | int) }}"
            unhealthy_clusters: "{{ get_cluster_statuses.results
                                | map(attribute='content')
                                | select('match', '^!ok$')
                                | list
                                | count }}"

        - name: Calculate how many replicas to run in this cluster
          set_fact:
            # Need to use python ternary syntax for lazy evaluation to avoid ZeroDivisionError
            # https://github.com/ansible/ansible/issues/38562
            this_clusters_replicas_active: "{{ 0 if (passive_cluster | bool)
                                               else (total_desired_replicas | int / healthy_clusters | int | round(0, 'ceil') | int) }}"

            # Only scale passive cluster up if all other clusters are unhealthy
            this_clusters_replicas_passive: "{{ 0 if (healthy_clusters != 0)
                                                else total_desired_replicas | int }}"

        - name: Set replica count for this cluster
          set_fact:
            this_clusters_replicas: "{{ this_clusters_replicas_passive if (passive_cluster | bool)
                                        else this_clusters_replicas_active }}"

        - name: Log in to this cluster
          shell: "/usr/bin/oc
                  login
                  --insecure-skip-tls-verify
                  --token={{ sa_token }}
                  --server={{ this_cluster_api_url }}"
          no_log: true
          changed_when: false

        - name: Get current application scale
          shell: "/usr/bin/oc
                  get
                  {{ deployment_or_deploymentconfig }}/{{ deployment_name }}
                  -o yaml
                  -n {{ project_name }}"
          changed_when: false
          register: oc_get_dc

        - name: Print job analysis details
          debug:
            msg:
            - "This cluster is configured: {{ passive_cluster | ternary('PASSIVE', 'ACTIVE') }}"
            - "Healthy Clusters: {{ healthy_clusters }}"
            - "Unhealthy Clusters: {{ unhealthy_clusters }}"
            - "Required application replicas in this cluster: {{ this_clusters_replicas | int }}"
            - "Current application replicas in this cluster: {{ (oc_get_dc.stdout | from_yaml).status.replicas }}"

        # This serves both to adjust scale up when clusters are unhealthy
        # or adjust scale down when all clusters are healthy again
        - name: Scale application
          shell: "/usr/bin/oc
                  scale
                  {{ deployment_or_deploymentconfig }}/{{ deployment_name }}
                  --replicas {{ this_clusters_replicas | int }}
                  -n {{ project_name }}"
          when: (oc_get_dc.stdout | from_yaml).status.replicas | int != this_clusters_replicas | int

# ServiceAccount
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}
    namespace: ${DEPLOYMENT_PROJECT}
    labels:
      app.kubernetes.io/name: ha-cronjob
      app.kubernetes.io/instance: ha-cronjob-${DEPLOYMENT_NAME}
      app.kubernetes.io/version: 1.0.0
      app.kubernetes.io/component: serviceaccount
      app.kubernetes.io/part-of: ${DEPLOYMENT_NAME}
      app.kubernetes.io/managed-by: openshift

# RoleBinding
- apiVersion: authorization.openshift.io/v1
  kind: RoleBinding
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}-edit
    namespace: ${DEPLOYMENT_PROJECT}
  roleRef:
    name: edit
  subjects:
  - kind: SystemUser
    name: system:serviceaccount:${DEPLOYMENT_PROJECT}:ha-cronjob-${DEPLOYMENT_NAME}
  userNames:
  - system:serviceaccount:${DEPLOYMENT_PROJECT}:ha-cronjob-${DEPLOYMENT_NAME}

# CronJob
- apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: ha-cronjob-${DEPLOYMENT_NAME}
    namespace: ${DEPLOYMENT_PROJECT}
    labels:
      app.kubernetes.io/name: ha-cronjob
      app.kubernetes.io/instance: ha-cronjob-${DEPLOYMENT_NAME}
      app.kubernetes.io/version: 1.0.0
      app.kubernetes.io/component: cronjob
      app.kubernetes.io/part-of: ${DEPLOYMENT_NAME}
      app.kubernetes.io/managed-by: openshift
  spec:
    concurrencyPolicy: Forbid
    # Run every minute
    schedule: '* * * * *'
    jobTemplate:
      spec:
        template:
          spec:
            containers:
            - name: job
              image: docker-registry.default.svc:5000/${DEPLOYMENT_PROJECT}/ha-cronjob-${DEPLOYMENT_NAME}:latest
              env:
              - name: THIS_CLUSTER_API_URL
                value: ${THIS_CLUSTER_API_URL}
              - name: ALTERNATE_CLUSTER_API_URL_LIST
                value: ${ALTERNATE_CLUSTER_API_URL_LIST}
              - name: DEPLOYMENT_OR_DEPLOYMENTCONFIG
                value: ${DEPLOYMENT_OR_DEPLOYMENTCONFIG}
              - name: DEPLOYMENT_NAME
                value: ${DEPLOYMENT_NAME}
              - name: PROJECT_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: TOTAL_DESIRED_REPLICAS
                value: ${TOTAL_DESIRED_REPLICAS}
              # https://docs.ansible.com/ansible/latest/reference_appendices/config.html#ansible-configuration-settings
              - name: ANSIBLE_LOCALHOST_WARNING
                value: "False"
              command:
              - '/jobscripts/job'
              workingDir: /tmp
              volumeMounts:
              - name: job-scripts
                mountPath: /jobscripts
            serviceAccountName: ha-cronjob-${DEPLOYMENT_NAME}
            volumes:
            - name: job-scripts
              configMap:
                name: ha-cronjob-${DEPLOYMENT_NAME}
                items:
                - key: job
                  path: job
                  mode: 0777
            restartPolicy: Never
